"""
Pipeline ETL Simple - Extraction, Transformation, Chargement
Auteur: Freud GUEDOU
Date: Octobre 2024

Pipeline automatis√© qui extrait des donn√©es CSV, les transforme, 
les valide et les charge dans une base de donn√©es SQLite.
"""

import pandas as pd
import sqlite3
import logging
from datetime import datetime
import os
import re

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('etl_pipeline.log'),
        logging.StreamHandler()
    ]
)

class ETLPipeline:
    """Classe principale du pipeline ETL"""
    
    def __init__(self, db_name='data_warehouse.db'):
        """
        Initialise le pipeline ETL
        
        Args:
            db_name (str): Nom de la base de donn√©es SQLite
        """
        self.db_name = db_name
        self.connection = None
        self.stats = {
            'extracted': 0,
            'transformed': 0,
            'loaded': 0,
            'errors': 0
        }
        
    def connect_db(self):
        """√âtablit la connexion √† la base de donn√©es"""
        try:
            self.connection = sqlite3.connect(self.db_name)
            logging.info(f"‚úÖ Connexion √©tablie √† la base de donn√©es: {self.db_name}")
        except Exception as e:
            logging.error(f"‚ùå Erreur de connexion √† la base de donn√©es: {e}")
            raise
    
    def close_db(self):
        """Ferme la connexion √† la base de donn√©es"""
        if self.connection:
            self.connection.close()
            logging.info("üîí Connexion √† la base de donn√©es ferm√©e")
    
    # ==================== EXTRACTION ====================
    
    def extract_csv(self, file_path, encoding='utf-8'):
        """
        Extrait les donn√©es d'un fichier CSV
        
        Args:
            file_path (str): Chemin du fichier CSV
            encoding (str): Encodage du fichier
            
        Returns:
            pd.DataFrame: DataFrame contenant les donn√©es extraites
        """
        try:
            logging.info(f"üì• Extraction des donn√©es depuis: {file_path}")
            
            if not os.path.exists(file_path):
                raise FileNotFoundError(f"Fichier non trouv√©: {file_path}")
            
            df = pd.read_csv(file_path, encoding=encoding)
            self.stats['extracted'] = len(df)
            
            logging.info(f"‚úÖ {len(df)} lignes extraites")
            logging.info(f"üìä Colonnes: {', '.join(df.columns.tolist())}")
            
            return df
            
        except Exception as e:
            logging.error(f"‚ùå Erreur lors de l'extraction: {e}")
            self.stats['errors'] += 1
            raise
    
    # ==================== TRANSFORMATION ====================
    
    def clean_data(self, df):
        """
        Nettoie les donn√©es (suppression des doublons, valeurs nulles)
        
        Args:
            df (pd.DataFrame): DataFrame √† nettoyer
            
        Returns:
            pd.DataFrame: DataFrame nettoy√©
        """
        logging.info("üßπ Nettoyage des donn√©es...")
        
        initial_count = len(df)
        
        # Supprimer les doublons
        df_clean = df.drop_duplicates()
        duplicates_removed = initial_count - len(df_clean)
        
        if duplicates_removed > 0:
            logging.info(f"   ‚ûú {duplicates_removed} doublons supprim√©s")
        
        # Compter les valeurs nulles
        null_counts = df_clean.isnull().sum()
        if null_counts.sum() > 0:
            logging.info(f"   ‚ûú Valeurs nulles d√©tect√©es: {null_counts[null_counts > 0].to_dict()}")
        
        return df_clean
    
    def validate_data(self, df, rules):
        """
        Valide les donn√©es selon des r√®gles d√©finies
        
        Args:
            df (pd.DataFrame): DataFrame √† valider
            rules (dict): R√®gles de validation
            
        Returns:
            pd.DataFrame: DataFrame valid√©
        """
        logging.info("‚úîÔ∏è  Validation des donn√©es...")
        
        df_valid = df.copy()
        rows_before = len(df_valid)
        
        for column, rule in rules.items():
            if column not in df_valid.columns:
                logging.warning(f"   ‚ö†Ô∏è  Colonne '{column}' non trouv√©e")
                continue
            
            if rule['type'] == 'not_null':
                mask = df_valid[column].notna()
                removed = (~mask).sum()
                df_valid = df_valid[mask]
                if removed > 0:
                    logging.info(f"   ‚ûú {removed} lignes supprim√©es ({column} null)")
            
            elif rule['type'] == 'range':
                mask = (df_valid[column] >= rule['min']) & (df_valid[column] <= rule['max'])
                removed = (~mask).sum()
                df_valid = df_valid[mask]
                if removed > 0:
                    logging.info(f"   ‚ûú {removed} lignes hors plage ({column})")
            
            elif rule['type'] == 'pattern':
                mask = df_valid[column].astype(str).str.match(rule['regex'])
                removed = (~mask).sum()
                df_valid = df_valid[mask]
                if removed > 0:
                    logging.info(f"   ‚ûú {removed} lignes avec format invalide ({column})")
        
        rows_after = len(df_valid)
        total_removed = rows_before - rows_after
        
        if total_removed > 0:
            logging.info(f"   ‚ûú Total: {total_removed} lignes supprim√©es apr√®s validation")
        
        return df_valid
    
    def transform_data(self, df, transformations):
        """
        Applique des transformations aux donn√©es
        
        Args:
            df (pd.DataFrame): DataFrame √† transformer
            transformations (dict): Transformations √† appliquer
            
        Returns:
            pd.DataFrame: DataFrame transform√©
        """
        logging.info("üîÑ Transformation des donn√©es...")
        
        df_transformed = df.copy()
        
        for column, transform in transformations.items():
            if column not in df_transformed.columns:
                logging.warning(f"   ‚ö†Ô∏è  Colonne '{column}' non trouv√©e")
                continue
            
            if transform['type'] == 'uppercase':
                df_transformed[column] = df_transformed[column].str.upper()
                logging.info(f"   ‚ûú {column}: converti en majuscules")
            
            elif transform['type'] == 'lowercase':
                df_transformed[column] = df_transformed[column].str.lower()
                logging.info(f"   ‚ûú {column}: converti en minuscules")
            
            elif transform['type'] == 'date':
                df_transformed[column] = pd.to_datetime(df_transformed[column], errors='coerce')
                logging.info(f"   ‚ûú {column}: converti en date")
            
            elif transform['type'] == 'category':
                df_transformed[column] = df_transformed[column].astype('category')
                logging.info(f"   ‚ûú {column}: converti en cat√©gorie")
            
            elif transform['type'] == 'strip':
                df_transformed[column] = df_transformed[column].str.strip()
                logging.info(f"   ‚ûú {column}: espaces supprim√©s")
            
            elif transform['type'] == 'calculate':
                df_transformed[column] = eval(transform['formula'])
                logging.info(f"   ‚ûú {column}: calcul√© ({transform['formula']})")
        
        self.stats['transformed'] = len(df_transformed)
        return df_transformed
    
    # ==================== CHARGEMENT ====================
    
    def load_to_db(self, df, table_name, if_exists='replace'):
        """
        Charge les donn√©es dans la base de donn√©es SQLite
        
        Args:
            df (pd.DataFrame): DataFrame √† charger
            table_name (str): Nom de la table
            if_exists (str): Action si la table existe ('replace', 'append', 'fail')
        """
        try:
            logging.info(f"üíæ Chargement des donn√©es dans la table: {table_name}")
            
            df.to_sql(table_name, self.connection, if_exists=if_exists, index=False)
            self.stats['loaded'] = len(df)
            
            logging.info(f"‚úÖ {len(df)} lignes charg√©es dans '{table_name}'")
            
            # Cr√©er des index pour am√©liorer les performances
            self.create_indexes(table_name, df.columns.tolist())
            
        except Exception as e:
            logging.error(f"‚ùå Erreur lors du chargement: {e}")
            self.stats['errors'] += 1
            raise
    
    def create_indexes(self, table_name, columns):
        """
        Cr√©e des index sur les colonnes principales
        
        Args:
            table_name (str): Nom de la table
            columns (list): Liste des colonnes
        """
        try:
            cursor = self.connection.cursor()
            
            # Cr√©er un index sur la premi√®re colonne (souvent l'ID)
            if columns:
                index_name = f"idx_{table_name}_{columns[0]}"
                cursor.execute(f"CREATE INDEX IF NOT EXISTS {index_name} ON {table_name}({columns[0]})")
                logging.info(f"   ‚ûú Index cr√©√©: {index_name}")
            
            self.connection.commit()
            
        except Exception as e:
            logging.warning(f"   ‚ö†Ô∏è  Erreur cr√©ation index: {e}")
    
    # ==================== PIPELINE COMPLET ====================
    
    def run_pipeline(self, csv_file, table_name, validation_rules=None, transformations=None):
        """
        Execute le pipeline ETL complet
        
        Args:
            csv_file (str): Chemin du fichier CSV
            table_name (str): Nom de la table de destination
            validation_rules (dict): R√®gles de validation (optionnel)
            transformations (dict): Transformations √† appliquer (optionnel)
        """
        start_time = datetime.now()
        
        logging.info("="*70)
        logging.info("üöÄ D√âMARRAGE DU PIPELINE ETL")
        logging.info("="*70)
        
        try:
            # Connexion √† la base de donn√©es
            self.connect_db()
            
            # EXTRACT
            df = self.extract_csv(csv_file)
            
            # TRANSFORM
            df = self.clean_data(df)
            
            if validation_rules:
                df = self.validate_data(df, validation_rules)
            
            if transformations:
                df = self.transform_data(df, transformations)
            
            # LOAD
            self.load_to_db(df, table_name)
            
            # Statistiques finales
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            logging.info("="*70)
            logging.info("üìä STATISTIQUES DU PIPELINE")
            logging.info("="*70)
            logging.info(f"‚úÖ Lignes extraites:    {self.stats['extracted']}")
            logging.info(f"üîÑ Lignes transform√©es: {self.stats['transformed']}")
            logging.info(f"üíæ Lignes charg√©es:     {self.stats['loaded']}")
            logging.info(f"‚ùå Erreurs:             {self.stats['errors']}")
            logging.info(f"‚è±Ô∏è  Dur√©e:               {duration:.2f} secondes")
            logging.info("="*70)
            logging.info("‚úÖ PIPELINE TERMIN√â AVEC SUCC√àS!")
            logging.info("="*70)
            
        except Exception as e:
            logging.error(f"‚ùå PIPELINE √âCHOU√â: {e}")
            raise
        
        finally:
            self.close_db()


def main():
    """Fonction principale - exemple d'utilisation"""
    
    # Configuration du pipeline
    pipeline = ETLPipeline(db_name='data_warehouse.db')
    
    # D√©finir les r√®gles de validation
    validation_rules = {
        'email': {
            'type': 'pattern',
            'regex': r'^[\w\.-]+@[\w\.-]+\.\w+$'
        },
        'age': {
            'type': 'range',
            'min': 18,
            'max': 100
        }
    }
    
    # D√©finir les transformations
    transformations = {
        'nom': {'type': 'uppercase'},
        'email': {'type': 'lowercase'},
        'ville': {'type': 'strip'},
        'date_inscription': {'type': 'date'}
    }
    
    # Ex√©cuter le pipeline
    pipeline.run_pipeline(
        csv_file='data/clients.csv',
        table_name='clients',
        validation_rules=validation_rules,
        transformations=transformations
    )


if __name__ == "__main__":
    main()
