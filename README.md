# ğŸ”„ Pipeline ETL Simple

**Auteur:** Freud GUEDOU  
**Date:** Octobre 2024

## ğŸ“‹ Description

Pipeline ETL (Extract, Transform, Load) automatisÃ© en Python qui extrait des donnÃ©es depuis des fichiers CSV, les transforme, les valide et les charge dans une base de donnÃ©es SQLite. Ce projet dÃ©montre les concepts fondamentaux de l'intÃ©gration de donnÃ©es et de la Business Intelligence.

## ğŸ¯ Objectifs

- âœ… **Extraction** : Lecture automatique de fichiers CSV
- âœ… **Transformation** : Nettoyage, validation et enrichissement des donnÃ©es
- âœ… **Chargement** : Insertion dans une base de donnÃ©es SQLite
- âœ… **Validation** : VÃ©rification de l'intÃ©gritÃ© des donnÃ©es
- âœ… **Logging** : TraÃ§abilitÃ© complÃ¨te des opÃ©rations
- âœ… **RequÃªtes** : Analyse des donnÃ©es chargÃ©es

## ğŸ› ï¸ Technologies utilisÃ©es

- **Python 3.8+** - Langage principal
- **Pandas** - Manipulation de donnÃ©es
- **SQLite3** - Base de donnÃ©es embarquÃ©e
- **Logging** - TraÃ§abilitÃ© des opÃ©rations

## ğŸ“Š FonctionnalitÃ©s du Pipeline

### Extraction (Extract)
- Lecture de fichiers CSV avec gestion d'encodage
- DÃ©tection automatique des colonnes
- Logging des donnÃ©es extraites

### Transformation (Transform)
- **Nettoyage** : Suppression des doublons et valeurs nulles
- **Validation** : VÃ©rification des formats (email, Ã¢ge, etc.)
- **Transformation** : Conversion de types, normalisation
- **Enrichissement** : Calculs de champs dÃ©rivÃ©s

### Chargement (Load)
- Insertion dans SQLite
- CrÃ©ation automatique de tables
- Indexation pour performances
- Gestion des erreurs

## ğŸš€ Installation

```bash
# Cloner le dÃ©pÃ´t
git clone https://github.com/votre-username/pipeline-etl-simple.git
cd pipeline-etl-simple

# CrÃ©er un environnement virtuel (recommandÃ©)
python -m venv venv
source venv/bin/activate  # Sur Windows: venv\Scripts\activate

# Installer les dÃ©pendances
pip install -r requirements.txt
```

## ğŸ’» Utilisation

### Ã‰tape 1 : GÃ©nÃ©rer des donnÃ©es d'exemple

```bash
python generate_sample_data.py
```

CrÃ©e 2 fichiers CSV :
- `data/clients.csv` (1000+ lignes)
- `data/ventes.csv` (500 lignes)

### Ã‰tape 2 : ExÃ©cuter le pipeline ETL

```bash
python etl_pipeline.py
```

Le pipeline va :
1. Extraire les donnÃ©es du CSV
2. Nettoyer et valider les donnÃ©es
3. Transformer selon les rÃ¨gles dÃ©finies
4. Charger dans la base de donnÃ©es SQLite

### Ã‰tape 3 : Analyser les donnÃ©es

```bash
python query_database.py
```

GÃ©nÃ¨re un rapport complet avec :
- Statistiques gÃ©nÃ©rales
- Top clients/produits
- RÃ©partitions par catÃ©gories

## ğŸ“ Structure du projet

```
pipeline-etl-simple/
â”‚
â”œâ”€â”€ etl_pipeline.py              # Pipeline ETL principal
â”œâ”€â”€ generate_sample_data.py      # GÃ©nÃ©rateur de donnÃ©es test
â”œâ”€â”€ query_database.py            # RequÃªtes et analyses
â”œâ”€â”€ requirements.txt             # DÃ©pendances Python
â”œâ”€â”€ README.md                    # Documentation
â”‚
â”œâ”€â”€ data/                        # DonnÃ©es CSV (gÃ©nÃ©rÃ©)
â”‚   â”œâ”€â”€ clients.csv
â”‚   â””â”€â”€ ventes.csv
â”‚
â”œâ”€â”€ data_warehouse.db            # Base de donnÃ©es SQLite (gÃ©nÃ©rÃ©)
â””â”€â”€ etl_pipeline.log             # Logs du pipeline (gÃ©nÃ©rÃ©)
```

## ğŸ“ˆ Exemple de sortie

### Pipeline ETL
```
======================================================================
ğŸš€ DÃ‰MARRAGE DU PIPELINE ETL
======================================================================
âœ… Connexion Ã©tablie Ã  la base de donnÃ©es: data_warehouse.db
ğŸ“¥ Extraction des donnÃ©es depuis: data/clients.csv
âœ… 1050 lignes extraites
ğŸ§¹ Nettoyage des donnÃ©es...
   âœ 46 doublons supprimÃ©s
âœ”ï¸  Validation des donnÃ©es...
   âœ 81 lignes avec format invalide (email)
   âœ 28 lignes hors plage (age)
ğŸ”„ Transformation des donnÃ©es...
   âœ nom: converti en majuscules
   âœ email: converti en minuscules
ğŸ’¾ Chargement des donnÃ©es dans la table: clients
âœ… 895 lignes chargÃ©es dans 'clients'
======================================================================
ğŸ“Š STATISTIQUES DU PIPELINE
======================================================================
âœ… Lignes extraites:    1050
ğŸ”„ Lignes transformÃ©es: 895
ğŸ’¾ Lignes chargÃ©es:     895
âŒ Erreurs:             0
â±ï¸  DurÃ©e:               0.05 secondes
======================================================================
```

## ğŸ”§ Personnalisation

### Ajouter des rÃ¨gles de validation

```python
validation_rules = {
    'email': {
        'type': 'pattern',
        'regex': r'^[\w\.-]+@[\w\.-]+\.\w+$'
    },
    'age': {
        'type': 'range',
        'min': 18,
        'max': 100
    }
}
```

### Ajouter des transformations

```python
transformations = {
    'nom': {'type': 'uppercase'},
    'email': {'type': 'lowercase'},
    'date_inscription': {'type': 'date'}
}
```

### Traiter vos propres donnÃ©es

```python
pipeline = ETLPipeline('votre_base.db')
pipeline.run_pipeline(
    csv_file='vos_donnees.csv',
    table_name='votre_table',
    validation_rules=vos_regles,
    transformations=vos_transformations
)
```

## ğŸ“Š RequÃªtes SQL disponibles

Le script `query_database.py` inclut des requÃªtes prÃ©dÃ©finies :

- **Clients** : Top clients, rÃ©partition par ville/statut
- **Ventes** : Chiffre d'affaires, produits populaires
- **Analyses** : Tendances mensuelles, statistiques

## ğŸ“ Concepts dÃ©montrÃ©s

### ETL
- Pipeline de donnÃ©es complet
- Extraction depuis sources multiples
- Transformations complexes
- Chargement optimisÃ©

### Data Quality
- Validation de donnÃ©es
- Nettoyage automatique
- Gestion des erreurs

### SQL & Bases de donnÃ©es
- SQLite embarquÃ©
- CrÃ©ation de schÃ©mas
- Indexation
- RequÃªtes analytiques

### Logging & Monitoring
- TraÃ§abilitÃ© complÃ¨te
- Fichiers de logs
- Statistiques dÃ©taillÃ©es

## ğŸ” Cas d'usage

Ce pipeline ETL peut Ãªtre adaptÃ© pour :

- Migration de donnÃ©es entre systÃ¨mes
- Consolidation de donnÃ©es de sources multiples
- Automatisation de rapports
- Data warehousing
- PrÃ©paration de donnÃ©es pour analyse

## ğŸ¤ Contribution

Les contributions sont bienvenues !

1. Fork le projet
2. CrÃ©er une branche (`git checkout -b feature/amelioration`)
3. Commit les changements (`git commit -m 'Ajout fonctionnalitÃ©'`)
4. Push vers la branche (`git push origin feature/amelioration`)
5. Ouvrir une Pull Request

## ğŸ“ Licence

Ce projet est sous licence MIT. Voir le fichier [LICENSE](LICENSE) pour plus de dÃ©tails.

## ğŸ‘¤ Auteur

**Freud GUEDOU**
- Projet personnel de Business Intelligence
- SpÃ©cialisation : ETL, Data Engineering, Python
- Date : Octobre 2024

## ğŸ“š Ressources

- [Documentation Pandas](https://pandas.pydata.org/)
- [SQLite Tutorial](https://www.sqlitetutorial.net/)
- [Python Logging](https://docs.python.org/3/library/logging.html)

---

*Projet rÃ©alisÃ© dans le cadre d'un apprentissage en Business Intelligence et Data Engineering*

**â­ Si ce projet vous est utile, n'hÃ©sitez pas Ã  lui donner une Ã©toile sur GitHub !**
